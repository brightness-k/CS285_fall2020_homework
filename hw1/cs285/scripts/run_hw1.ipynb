{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')#to test different models change \"CartPole-V0\" to different models\n",
    "env.reset()\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UunygyDXrx7k"
   },
   "source": [
    "## Run Behavior Cloning (Problem 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "enh5ZMHftEO7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#@title imports\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from cs285.infrastructure.rl_trainer import RL_Trainer\n",
    "from cs285.agents.bc_agent import BCAgent\n",
    "from cs285.policies.loaded_gaussian_policy import LoadedGaussianPolicy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "imnAkQ6jryL7"
   },
   "outputs": [],
   "source": [
    "#@title runtime arguments\n",
    "\n",
    "class Args:\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "    return getattr(self, key)\n",
    "\n",
    "  def __setitem__(self, key, val):\n",
    "    setattr(self, key, val)\n",
    "\n",
    "  #@markdown expert data\n",
    "  expert_policy_file = 'cs285/policies/experts/Ant.pkl' #@param\n",
    "  expert_data = 'cs285/expert_data/expert_data_Ant-v2.pkl' #@param\n",
    "  env_name = 'Ant-v2' # ['Ant-v2', 'Humanoid-v2', 'Walker2d-v2', 'HalfCheetah-v2', 'Hopper-v2']\n",
    "  exp_name = 'test_bc_Ant' #@param\n",
    "  do_dagger = True #@param {type: \"boolean\"}\n",
    "  ep_len = 1000 #@param {type: \"integer\"}\n",
    "  save_params = True #@param {type: \"boolean\"}\n",
    "\n",
    "  num_agent_train_steps_per_iter = 1000 #@param {type: \"integer\"})\n",
    "  n_iter = 10 #@param {type: \"integer\"})\n",
    "\n",
    "  #@markdown batches & buffers\n",
    "  batch_size = 1000 #@param {type: \"integer\"})\n",
    "  eval_batch_size = 1000 #@param {type: \"integer\"}\n",
    "  train_batch_size = 100 #@param {type: \"integer\"}\n",
    "  max_replay_buffer_size = 1000000 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown network\n",
    "  n_layers = 2 #@param {type: \"integer\"}\n",
    "  size = 64 #@param {type: \"integer\"}\n",
    "  learning_rate = 5e-3 #@param {type: \"number\"}\n",
    "\n",
    "  #@markdown logging\n",
    "  video_log_freq = 1 #@param {type: \"integer\"}\n",
    "  scalar_log_freq = 1 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown gpu & run-time settings\n",
    "  no_gpu = False #@param {type: \"boolean\"}\n",
    "  which_gpu = 0 #@param {type: \"integer\"}\n",
    "  seed = 1 #@param {type: \"integer\"}\n",
    "\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "fLnU1evmss4I"
   },
   "outputs": [],
   "source": [
    "#@title define `BC_Trainer`\n",
    "class BC_Trainer(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        #######################\n",
    "        ## AGENT PARAMS\n",
    "        #######################\n",
    "\n",
    "        agent_params = {\n",
    "            'n_layers': params['n_layers'],\n",
    "            'size': params['size'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'max_replay_buffer_size': params['max_replay_buffer_size'],\n",
    "            }\n",
    "\n",
    "        self.params = params\n",
    "        self.params['agent_class'] = BCAgent ## TODO: look in here and implement this\n",
    "        self.params['agent_params'] = agent_params\n",
    "\n",
    "        ################\n",
    "        ## RL TRAINER\n",
    "        ################\n",
    "\n",
    "        self.rl_trainer = RL_Trainer(self.params) ## TODO: look in here and implement this\n",
    "\n",
    "        #######################\n",
    "        ## LOAD EXPERT POLICY\n",
    "        #######################\n",
    "\n",
    "        print('Loading expert policy from...', self.params['expert_policy_file'])\n",
    "        self.loaded_expert_policy = LoadedGaussianPolicy(self.params['expert_policy_file'])\n",
    "        print('Done restoring expert policy...')\n",
    "\n",
    "    def run_training_loop(self):\n",
    "        self.rl_trainer.run_training_loop(\n",
    "            n_iter=self.params['n_iter'],\n",
    "            initial_expertdata=self.params['expert_data'],\n",
    "            collect_policy=self.rl_trainer.agent.actor,\n",
    "            eval_policy=self.rl_trainer.agent.actor,\n",
    "            relabel_with_expert=self.params['do_dagger'],\n",
    "            expert_policy=self.loaded_expert_policy,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "7UkzHBfxsxH8"
   },
   "outputs": [],
   "source": [
    "#@title create directory for logging\n",
    "\n",
    "if args.do_dagger:\n",
    "    logdir_prefix = 'q2_'  # The autograder uses the prefix `q2_`\n",
    "    assert args.n_iter>1, ('DAgger needs more than 1 iteration (n_iter>1) of training, to iteratively query the expert and train (after 1st warmstarting from behavior cloning).')\n",
    "else:\n",
    "    logdir_prefix = 'q1_'  # The autograder uses the prefix `q1_`\n",
    "    assert args.n_iter==1, ('Vanilla behavior cloning collects expert data just once (n_iter=1)')\n",
    "\n",
    "data_path ='./content/cs285_f2020/data'\n",
    "if not (os.path.exists(data_path)):\n",
    "    os.makedirs(data_path)\n",
    "logdir = logdir_prefix + args.exp_name + '_' + args.env_name + \\\n",
    "         '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "logdir = os.path.join(data_path, logdir)\n",
    "args['logdir'] = logdir\n",
    "if not(os.path.exists(logdir)):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_qQb789_syt0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "./content/cs285_f2020/data\\q2_test_bc_Ant_Ant-v2_20-04-2021_18-17-05\n",
      "########################\n",
      "logging outputs to  ./content/cs285_f2020/data\\q2_test_bc_Ant_Ant-v2_20-04-2021_18-17-05\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4260.87353515625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 4260.87353515625\n",
      "Eval_MinReturn : 4260.87353515625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4713.6533203125\n",
      "Train_StdReturn : 12.196533203125\n",
      "Train_MaxReturn : 4725.849609375\n",
      "Train_MinReturn : 4701.45654296875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 1.7527616024017334\n",
      "Training Loss : 0.002412128262221813\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "Saving agent params\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting train rollouts to be used for saving videos...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Saving train rollouts as videos...\n",
      "Eval_AverageReturn : 4863.72021484375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 4863.72021484375\n",
      "Eval_MinReturn : 4863.72021484375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4274.9248046875\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4274.9248046875\n",
      "Train_MinReturn : 4274.9248046875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 1000\n",
      "TimeSinceStart : 10.553403615951538\n",
      "Training Loss : 0.0018981273751705885\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "Saving agent params\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting train rollouts to be used for saving videos...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Saving train rollouts as videos...\n",
      "Eval_AverageReturn : 4791.927734375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 4791.927734375\n",
      "Eval_MinReturn : 4791.927734375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4697.6533203125\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4697.6533203125\n",
      "Train_MinReturn : 4697.6533203125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 2000\n",
      "TimeSinceStart : 21.49313187599182\n",
      "Training Loss : 0.0011719836620613933\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "Saving agent params\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting train rollouts to be used for saving videos...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Saving train rollouts as videos...\n",
      "Eval_AverageReturn : 4740.54296875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 4740.54296875\n",
      "Eval_MinReturn : 4740.54296875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4566.8388671875\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4566.8388671875\n",
      "Train_MinReturn : 4566.8388671875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 3000\n",
      "TimeSinceStart : 32.511112689971924\n",
      "Training Loss : 0.0012669623829424381\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "Saving agent params\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting train rollouts to be used for saving videos...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Saving train rollouts as videos...\n",
      "Eval_AverageReturn : 4504.64404296875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 4504.64404296875\n",
      "Eval_MinReturn : 4504.64404296875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4674.59423828125\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4674.59423828125\n",
      "Train_MinReturn : 4674.59423828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 4000\n",
      "TimeSinceStart : 43.59065079689026\n",
      "Training Loss : 0.0009163652430288494\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "Saving agent params\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting train rollouts to be used for saving videos...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Saving train rollouts as videos...\n",
      "Eval_AverageReturn : 4713.2578125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 4713.2578125\n",
      "Eval_MinReturn : 4713.2578125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4677.98095703125\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4677.98095703125\n",
      "Train_MinReturn : 4677.98095703125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 5000\n",
      "TimeSinceStart : 54.85008215904236\n",
      "Training Loss : 0.0007612563786096871\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "Saving agent params\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting train rollouts to be used for saving videos...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Saving train rollouts as videos...\n",
      "Eval_AverageReturn : 4620.08447265625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 4620.08447265625\n",
      "Eval_MinReturn : 4620.08447265625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4716.2255859375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4716.2255859375\n",
      "Train_MinReturn : 4716.2255859375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 6000\n",
      "TimeSinceStart : 66.63061761856079\n",
      "Training Loss : 0.0008897619554772973\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "Saving agent params\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting train rollouts to be used for saving videos...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Saving train rollouts as videos...\n",
      "Eval_AverageReturn : 4558.2705078125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 4558.2705078125\n",
      "Eval_MinReturn : 4558.2705078125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4600.3701171875\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4600.3701171875\n",
      "Train_MinReturn : 4600.3701171875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 7000\n",
      "TimeSinceStart : 77.29679155349731\n",
      "Training Loss : 0.0005984592717140913\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "Saving agent params\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting train rollouts to be used for saving videos...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Saving train rollouts as videos...\n",
      "Eval_AverageReturn : 4720.580078125\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 4720.580078125\n",
      "Eval_MinReturn : 4720.580078125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4733.8037109375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4733.8037109375\n",
      "Train_MinReturn : 4733.8037109375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 8000\n",
      "TimeSinceStart : 88.05904006958008\n",
      "Training Loss : 0.000713261601049453\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "Saving agent params\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Collecting train rollouts to be used for saving videos...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "\n",
      "Collecting video rollouts eval\n",
      "\n",
      "Saving train rollouts as videos...\n",
      "Eval_AverageReturn : 4690.5234375\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 4690.5234375\n",
      "Eval_MinReturn : 4690.5234375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4649.3212890625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4649.3212890625\n",
      "Train_MinReturn : 4649.3212890625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 9000\n",
      "TimeSinceStart : 99.56010961532593\n",
      "Training Loss : 0.0005636814748868346\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "Saving agent params\n"
     ]
    }
   ],
   "source": [
    "## run training\n",
    "print(args.logdir)\n",
    "trainer = BC_Trainer(args)\n",
    "trainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75M0MlR5tUIb"
   },
   "outputs": [],
   "source": [
    "\n",
    "#@markdown You can visualize your runs with tensorboard from within the notebook\n",
    "\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir ./content/cs285_f2020/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ff9onuUPfPEa"
   },
   "source": [
    "## Running DAgger (Problem 2)\n",
    "Modify the settings above:\n",
    "1. check the `do_dagger` box\n",
    "2. set `n_iters` to `10`\n",
    "and then rerun the code."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "run_hw1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}